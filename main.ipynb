{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of software listing websites:\n",
      "https://cnct-ahmedraza.medium.com/15-b2b-software-listing-websites-for-backlinks-in-2023-9447b81758ce\n",
      "https://blog.2checkout.com/review-directories-for-listing-software-saas-products/\n",
      "https://research.com/software/g2-alternatives\n",
      "https://www.poweredbysearch.com/learn/top-b2b-saas-product-review-sites/\n",
      "https://www.capterra.com/legal/listing-guidelines/\n",
      "https://comradeweb.com/blog/top-33-best-b2b-websites/\n",
      "https://www.krishaweb.com/blog/top-b2b-directories-to-generate-quality-leads/\n",
      "https://b2bsaasreviews.com/best-software-review-sites/\n",
      "https://www.process.st/review-sites/\n",
      "https://b2bsaasreviews.com/best-b2b-services-review-sites/\n"
     ]
    }
   ],
   "source": [
    "#First i am using the google custom search API to fetch all the websites that give me a list of websites that list software products\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Function to fetch search results from Google Custom Search API\n",
    "def fetch_search_results(api_key, cx):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=\"b2b software listing websites\", cx=cx).execute()\n",
    "    return res.get('items', [])\n",
    "\n",
    "# Function to extract URLs from search results\n",
    "def extract_urls_from_results(results):\n",
    "    urls = []\n",
    "    for result in results:\n",
    "        urls.append(result['link'])\n",
    "    return urls\n",
    "\n",
    "# API key and CX (Custom Search Engine ID) obtained from Google Developer Console\n",
    "api_key = \"AIzaSyCdf6kt49E4fSHdeAnG1PgKQ-mdFbyS41o\"\n",
    "cx = \"c0436b72e0d7b48a2\"\n",
    "\n",
    "# Fetch search results\n",
    "search_results = fetch_search_results(api_key, cx)\n",
    "\n",
    "# Extract URLs from search results\n",
    "if search_results:\n",
    "    urls = extract_urls_from_results(search_results)\n",
    "    print(\"List of software listing websites:\")\n",
    "    for url in urls:\n",
    "        print(url)\n",
    "else:\n",
    "    print(\"No search results found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Free Business Listing Website']\n",
      "['www.indiamart.com']\n",
      "['www.indianyellowpages.com']\n",
      "['www.sulekha.com']\n",
      "['www.yalwa.in']\n",
      "['www.asklaila.com']\n",
      "['www.directories.net.in']\n",
      "['www.indyapages.com']\n",
      "['www.fundoodata.com']\n",
      "['www', '.', 'huntbiz', '.', 'com']\n",
      "['www.facebook.com<pages']\n",
      "['http://business.foursquare.com']\n",
      "['www.flyple.com']\n",
      "['www.indiabizlist.com']\n",
      "['www.workinfo.info']\n",
      "['www.justdial.com']\n",
      "['www.gbguides.com']\n",
      "['www.businessfinder.in']\n",
      "['www.linkedin.com']\n",
      "['www.smallbusiness.yahoo.com']\n",
      "['www.smartguy.com']\n",
      "['www.indiabusinesstoday.in']\n",
      "['https://indianbusinesscanada.com']\n",
      "['www.clickindia.com']\n",
      "['www.crunchbase.com']\n",
      "['www.freeadstime.org']\n",
      "['https://about.me']\n",
      "['www.yelp.com']\n",
      "['https://in.enrollbusiness.com']\n",
      "['www.yellowpagecity.com']\n",
      "['https://yellowpages.in']\n",
      "['www.yellowbot.com']\n",
      "['http://tupalo.com']\n",
      "['www.yellowpages.webindia123.com']\n",
      "['www.exportersindia.com']\n",
      "['www.expressbusinessdirectory.com']\n",
      "['www.smartguy.com']\n",
      "['https://in.enrollbusiness.com']\n",
      "['www.poweredindia.com']\n",
      "['www.amazon.in']\n",
      "['www.google.com/business']\n",
      "['https://sholay.in']\n",
      "['www.locanto.net']\n",
      "['www.reviewcentre.com']\n",
      "['www.slideshare.net']\n",
      "['www.mouthshut.com']\n",
      "['www.storeboard.com']\n",
      "['https://local.indiaonline.in']\n",
      "['www.salespider.com']\n",
      "['www.showmelocal.com']\n",
      "['www.urbanpro.com']\n",
      "['www.2findlocal.com']\n",
      "['www.opendi.in']\n",
      "['www.ezilon.com']\n",
      "['www.maharashtradirectory.com']\n",
      "['www.spoke.com']\n",
      "['www.gust.com']\n",
      "['www.communitywalk.com']\n",
      "['www.tuugo.in']\n",
      "['www.thetoptens.com']\n",
      "['www.cybo.com/india']\n",
      "['www.traderscity.com']\n",
      "['www.trepup.com']\n",
      "['www.gujaratdirectory.com']\n",
      "['www.indiabizclub.com']\n",
      "['www.indiacom.com']\n",
      "['www.eindiabusiness.com']\n",
      "['www.indiabook.com']\n",
      "['www.indiacatalog.com']\n",
      "['www.freelistingindia.in']\n",
      "['www.jantareview.com']\n",
      "['www.dialindia.com', '']\n",
      "['https://aaspass.com']\n",
      "['https://bharathlisting.com']\n",
      "['www.indiabusinessenquiry.com']\n",
      "['www.justbaazaar.com']\n",
      "['www.jimyellowpages.com']\n",
      "['www.tradeindia.com']\n",
      "['www.surfindia.com']\n",
      "['https://indianceo.in']\n",
      "['www.indiabook.com']\n",
      "['https://paperdoor.in']\n",
      "['www.localfrog.in']\n",
      "['www.snapdeal.com']\n",
      "['www.vanik.com']\n",
      "['www.zipleaf.com']\n",
      "['www.fullhyderabad.com']\n",
      "['http://entireindia.com']\n",
      "['https://businessistingplus.com']\n",
      "['www.rajb2b.com']\n",
      "['www.trustpilot.com']\n",
      "['www.localstar.org']\n",
      "['www.addressguru.in']\n",
      "['www.findinall.com']\n",
      "['www.citytadka.com']\n",
      "['www.swiggy.com']\n",
      "['www.justcityplace.com']\n",
      "['www.zomato.com']\n",
      "['www.mapquest.com']\n"
     ]
    }
   ],
   "source": [
    "#Next i am picking a few of these websites to scrape them to get a list of websites that lists software products\n",
    "#website 1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_table(url):\n",
    "    \"\"\"Scrapes a table from a website and returns the data as a list.\"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        headers = {\n",
    "            'authority': 'www.google.com',\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'accept-language': 'en-US,en;q=0.9',\n",
    "            'cache-control': 'max-age=0',\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the table\n",
    "        table = soup.find('table')\n",
    "\n",
    "        # Extract table data and store it in a list\n",
    "        table_data = []\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            # Skip the first cell (index 0) in each row\n",
    "            cells = row.find_all(['td', 'th'])[1]  # Exclude the first cell\n",
    "            row_data = [cell.text.strip() for cell in cells]\n",
    "            table_data.append(row_data)\n",
    "\n",
    "        return table_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping table from {url}: {e}\")\n",
    "\n",
    "# Example URL of the website containing the table\n",
    "url = \"https://iimskills.com/business-listing-sites/\"\n",
    "table_data = scrape_table(url)\n",
    "\n",
    "# Print the scraped data list\n",
    "for row in table_data:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15 B2B Software Listing Websites for Backlinks in 2023', '1. Capterra', '2. GetApp', '3. Software Advice', '4. SourceForge', '5. Slashdot', '6. SoftwareSuggest', '7. GoodFirms', '8. SaaSHub', '9. SaaSworthy', '10. Serchen', '11. Producthunt', '12. Crowdreviews', '13. Trustpilot', '14. Sitejabber', '15. TrustRadius', '16. Crozdesk']\n"
     ]
    }
   ],
   "source": [
    "#website 2\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "link = \"https://cnct-ahmedraza.medium.com/15-b2b-software-listing-websites-for-backlinks-in-2023-9447b81758ce\"\n",
    "headers = {\n",
    "    'authority': 'www.google.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "}\n",
    "page = requests.get(link, headers=headers)\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "soup.prettify()\n",
    "potentialnames = soup.find_all('h1')\n",
    "web_list=[]\n",
    "for heading in potentialnames:\n",
    "    # Extract the text content without the h3 tags\n",
    "    web_list.append(heading.text.strip())\n",
    "print(web_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Freeplane', 'ProjectLibre - Project Management', 'OpenProject', 'TestLink', 'OpenProj - Project Management', 'Kanboard', 'ProjeQtOr - Project Management Tool', 'Wekan', 'GanttProject', 'Task Coach', 'Focalboard', 'Group Office groupware and CRM', 'SuiteCRM', 'Portainer.io', 'vym - view your mind', 'cloc', 'Taskcafe', 'Rukovoditel - Project Management/CRM', 'dotProject', 'Feng Office: Project Management and more', 'Collabtive', 'Mpp Viewer', 'WebCollab', 'ZenTao project management software', ']project-open[ - Project Server']\n"
     ]
    }
   ],
   "source": [
    "# now i am picking one of the software listing websites and scraping it for software products listed on it\n",
    "link = \"https://sourceforge.net/directory/project-management/windows/\"\n",
    "headers = {\n",
    "    'authority': 'www.google.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "}\n",
    "page = requests.get(link, headers=headers)\n",
    "soup=BeautifulSoup(page.content,'html.parser')\n",
    "soup.prettify()\n",
    "potentialnames = soup.find_all('h3')\n",
    "prod_list=[]\n",
    "for heading in potentialnames:\n",
    "    prod_list.append(heading.text.strip())\n",
    "print(prod_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeplane - Not found on G2\n",
      "ProjectLibre - Project Management - Not found on G2\n",
      "OpenProject - Found on G2\n",
      "TestLink - Found on G2\n",
      "OpenProj - Project Management - Not found on G2\n",
      "Kanboard - Found on G2\n",
      "ProjeQtOr - Project Management Tool - Not found on G2\n",
      "Wekan - Found on G2\n",
      "GanttProject - Found on G2\n",
      "Task Coach - Not found on G2\n",
      "Focalboard - Not found on G2\n",
      "Group Office groupware and CRM - Not found on G2\n",
      "SuiteCRM - Found on G2\n",
      "Portainer.io - Not found on G2\n",
      "vym - view your mind - Not found on G2\n",
      "cloc - Found on G2\n",
      "Taskcafe - Not found on G2\n",
      "Rukovoditel - Project Management/CRM - Not found on G2\n",
      "dotProject - Not found on G2\n",
      "Feng Office: Project Management and more - Not found on G2\n",
      "Collabtive - Found on G2\n",
      "Mpp Viewer - Not found on G2\n",
      "WebCollab - Not found on G2\n",
      "ZenTao project management software - Not found on G2\n",
      "]project-open[ - Project Server - Not found on G2\n",
      "added to the file!\n"
     ]
    }
   ],
   "source": [
    "#Now i am using the G2 api key to check if the products that i obtained from scraping are present on the G2 website or not\n",
    "import requests\n",
    "\n",
    "# Replace with your actual G2 API key\n",
    "API_KEY = \"c9aa2392c9d0322d1d79a6353b9d92dbb6f14fce1aae97c04e28d18751e1fe45\"\n",
    "BASE_URL = \"https://data.g2.com/api/v1/products\"\n",
    "\n",
    "# Function to check if a product exists on G2\n",
    "def check_product_on_g2(product_name):\n",
    "  url = f\"{BASE_URL}?filter[name]={product_name}\"\n",
    "  headers = {\"Authorization\": f\"Token token={API_KEY}\"}\n",
    "  response = requests.get(url, headers=headers)\n",
    "\n",
    "  # Check for successful response (200 OK)\n",
    "  if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Check if there are any products in the response data\n",
    "    if data[\"data\"]:\n",
    "      return True  # Product found on G2\n",
    "    else:\n",
    "      return False  # Product not found on G2\n",
    "  else:\n",
    "    print(f\"Error: API request failed with status code: {response.status_code}\")\n",
    "    return None  # Handle API errors\n",
    "\n",
    "def write_unlisted_products(product_list):\n",
    "  # Choose your preferred method (database or CSV)\n",
    "  # Here's an example for writing to a CSV file\n",
    "  # print(\"entering the writing part\")\n",
    "  with open(\"unlisted_products.csv\", \"w\") as f:\n",
    "    # f.write(\"Product Name\\n\")\n",
    "    for product in product_list:\n",
    "      f.write(f\"{product}\\n\")\n",
    "\n",
    "noting2 = []\n",
    "# Check each product in the list\n",
    "for product_name in prod_list:\n",
    "  if check_product_on_g2(product_name):\n",
    "    print(f\"{product_name} - Found on G2\")\n",
    "    noting2.append(product_name)\n",
    "  else:\n",
    "    print(f\"{product_name} - Not found on G2\")\n",
    "\n",
    "write_unlisted_products(noting2)\n",
    "print(\"added to the file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
